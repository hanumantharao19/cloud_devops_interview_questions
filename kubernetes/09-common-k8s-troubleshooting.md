
### Question  
Your Kubernetes pod is stuck in the `CrashLoopBackOff` state. How would you troubleshoot and resolve this issue?

### Answer  
To troubleshoot a `CrashLoopBackOff`, I would check pod logs, container events, and resource limits. Common causes include application bugs, incorrect configs, failed dependencies, or OOM errors. Iâ€™d resolve the root cause based on these findings.


#### 1. ğŸ” Check Pod Status and Reason

```bash
kubectl get pod <pod-name> -n <namespace>
kubectl describe pod <pod-name> -n <namespace>
```

Look under **"Last State"**, **"Exit Code"**, and **"Events"** to understand whatâ€™s causing the container crash.

---

#### 2. ğŸ“„ View Container Logs

```bash
kubectl logs <pod-name> -c <container-name> --previous -n <namespace>
```

Use `--previous` to see logs from the last failed attempt. Youâ€™ll often find stack traces, errors like:

- `Connection refused`
- `File not found`
- `Segmentation fault`
- `Permission denied`

---

#### 3. âš™ï¸ Check for Configuration or Secret Issues

- Did the pod mount a ConfigMap or Secret thatâ€™s missing or misconfigured?
- Are environment variables or command-line arguments set incorrectly?

```bash
kubectl describe pod <pod-name>
```

---

#### 4. ğŸ“¦ Check for Missing Dependencies

- Is the container trying to connect to a service thatâ€™s not running?
- Is a database unavailable or unreachable?
- Are DNS entries resolving?

---

#### 5. ğŸ’½ Check Resource Constraints

```bash
kubectl describe pod <pod-name>
```

If it shows:
```
Reason: OOMKilled
```
Then the container exceeded memory limits. Youâ€™ll need to increase memory in the spec or optimize the application.

---

#### 6. ğŸ› Check Image, CMD, ENTRYPOINT

Sometimes the crash is because of:

- Wrong image version
- Entry command missing a binary
- Script missing execute permissions

You can test locally with Docker or `kubectl run` to isolate the issue.

---

#### 7. ğŸ§ª Use Ephemeral Container for Debugging (K8s v1.23+)

```bash
kubectl debug -it <pod-name> --image=busybox --target=<container-name>
```

You can inspect volumes, paths, env variables while the pod crashes repeatedly.

---

## Question
## Your Deployment Has `replicas: 3`, but Only 1 Pod Is Running â€” What Could Be Wrong?

### Answer  
There could be several reasons: resource constraints on nodes, scheduling issues, crashlooping pods, or affinity/taint restrictions that prevent pods from starting.


#### âœ… 1. **Check Pod Statuses**

Run:
```bash
kubectl get pods -l app=my-app
```

You might see:
- 1 Running
- 2 Pending / CrashLoopBackOff / ImagePullBackOff

---

#### âœ… 2. **Describe the Deployment and Pods**

```bash
kubectl describe deployment my-deployment
kubectl describe pod <pod-name>
```

Look for:
- Events at the bottom (e.g., â€œfailed schedulingâ€)
- Volume mounting errors

---

#### âœ… 3. **Check Node Capacity**

Maybe the other pods canâ€™t be scheduled due to insufficient **CPU/memory**.

Run:
```bash
kubectl describe nodes
```

If the nodes are out of resources, new pods wonâ€™t start.

---
